{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SOM_algorithm.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "FAwBuzdLT33l",
        "UoZZuYx4Q2fb",
        "WhgqgHVjRM_a",
        "b25nkJGYRS0z",
        "4P5csr9nSEyy",
        "CKG20jKtSMK-",
        "MP_8deceWPeZ"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zhengro/DL-Identification/blob/jaume/SOM_algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "PKJsutnMRgxz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "__author__ = 'Jaume Anguera Peris'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FAwBuzdLT33l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Read txt files from Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "ZE45YAI_T9wE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ebed0f9a-efde-474f-966f-9ccf52c50f53"
      },
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cDiEOkemUW3W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# After executing the cell above, all the data needed for this project \n",
        "# will be present in \"/content/drive/My Drive/Project - DL identification systems/Data/\"\n",
        "MAIN_PATH = \"/content/drive/My Drive/Project - DL identification systems/Data/\"\n",
        "DEFAULT_MAIN_FOLDER = MAIN_PATH + '500 users/data_len_64/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UoZZuYx4Q2fb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import necessary libraries"
      ]
    },
    {
      "metadata": {
        "id": "cwWdv5NLRBcl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "683y4UnjRIpt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import classes"
      ]
    },
    {
      "metadata": {
        "id": "WhgqgHVjRM_a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### SOM_algorithm.py"
      ]
    },
    {
      "metadata": {
        "id": "Wb2hP7scHz_a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SOM:\n",
        "\n",
        "    DEFAULT_NUM_EPOCHS = 200\n",
        "    DEFAULT_INIT_LEARN_RATE = 0.4\n",
        "\n",
        "    # Initialization\n",
        "    def __init__(self, net_dim_x, net_dim_y, num_features,\n",
        "                 num_epochs=DEFAULT_NUM_EPOCHS,\n",
        "                 init_learning_rate=DEFAULT_INIT_LEARN_RATE):\n",
        "        self.net_dimensions = np.array([net_dim_x,net_dim_y])\n",
        "        self.num_features = num_features\n",
        "        self.num_epochs = num_epochs\n",
        "        self.init_learning_rate = init_learning_rate\n",
        "        self.init_radius = min(self.net_dimensions[0],self.net_dimensions[1])\n",
        "        self.time_constant = num_epochs / np.log(self.init_radius)\n",
        "        self.generate_weight_matrix()\n",
        "\n",
        "    # Functions\n",
        "    def generate_weight_matrix(self):\n",
        "        self.net_weights = np.random.random((self.num_features,\n",
        "                                             self.net_dimensions[0],\n",
        "                                             self.net_dimensions[1]))\n",
        "\n",
        "    def train(self, inputData, verbose=True):\n",
        "\n",
        "        if verbose is True:\n",
        "            msg_interval = 10\n",
        "\n",
        "        for i in range(self.num_epochs):\n",
        "            radius = self.decay_radius(i)\n",
        "            learning_rate = self.decay_learning_rate(i)\n",
        "\n",
        "            if verbose and (i % msg_interval == 0):\n",
        "                print(\"Iteration %d out of %d\" % (i,self.num_epochs))\n",
        "                print(\"Radius = %.2f\" % radius)\n",
        "                print(\"Learning rate = %.2e\\n\" % learning_rate)\n",
        "\n",
        "            bmu_ind = self.predict_cluster(inputData)\n",
        "\n",
        "            for x in range(self.net_dimensions[0]):\n",
        "                for y in range(self.net_dimensions[1]):\n",
        "                    node_position = np.array([x, y])\n",
        "                    weight_k = self.net_weights[:, x, y].reshape(1, self.num_features)\n",
        "                    for sample_ind in range(inputData.shape[0]):\n",
        "                        distance_between_nodes = SOM.euclidean_dist(node_position, bmu_ind[sample_ind])\n",
        "                        if distance_between_nodes <= radius ** 2:\n",
        "                            input_vec = inputData[sample_ind,:]\n",
        "                            step_size = learning_rate * SOM.neighborhood_influence(distance_between_nodes, radius)\n",
        "                            updated_weight = weight_k + (step_size * (input_vec - weight_k))\n",
        "                            self.net_weights[:, x, y] = updated_weight.reshape(self.num_features,)\n",
        "\n",
        "\n",
        "    def predict_cluster(self, inputData, saveData=False, fileName='cluster_ind.txt'):\n",
        "        cluster_ind = []\n",
        "\n",
        "        for user in range(inputData.shape[0]):\n",
        "            input_vec = inputData[user, :]\n",
        "            cluster_ind.append(self.find_bmu(input_vec))\n",
        "\n",
        "        if saveData is True:\n",
        "            np.savetxt(fileName, cluster_ind, fmt='[%d,%d]', delimiter=',')\n",
        "\n",
        "        return cluster_ind\n",
        "\n",
        "    def decay_radius(self, iteration):\n",
        "        return( self.init_radius * np.exp(-iteration/self.time_constant) )\n",
        "\n",
        "    def decay_learning_rate(self, iteration):\n",
        "        return ( self.init_learning_rate * np.exp(-iteration/self.num_epochs) )\n",
        "\n",
        "    @staticmethod\n",
        "    def euclidean_dist(first_vector, second_vector):\n",
        "        return ( np.sum((first_vector - second_vector) ** 2) )\n",
        "\n",
        "    @staticmethod\n",
        "    def neighborhood_influence(distance, radius):\n",
        "        return ( np.exp(-distance/(2 * (radius ** 2))) )\n",
        "\n",
        "    def find_bmu(self, input_vec):\n",
        "        bmu_ind = np.array([0, 0])\n",
        "        bmu_init = self.net_weights[:, 0, 0].reshape(1, self.num_features)\n",
        "        min_dist = self.euclidean_dist(input_vec, bmu_init)\n",
        "\n",
        "        for x in range(self.net_dimensions[0]):\n",
        "            for y in range(self.net_dimensions[1]):\n",
        "                weight_k = self.net_weights[:, x, y].reshape(1, self.num_features)\n",
        "                distance_between_vectors = SOM.euclidean_dist(input_vec, weight_k)\n",
        "                if min_dist > distance_between_vectors:\n",
        "                    min_dist = distance_between_vectors\n",
        "                    bmu_ind = np.array([x, y])\n",
        "\n",
        "        return bmu_ind\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b25nkJGYRS0z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### read_Data.py"
      ]
    },
    {
      "metadata": {
        "id": "ZOPDYI_4RILv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class dataReader:\n",
        "\t\"\"\"\n",
        "\tThis class is responsible for reading the data that will be used for\n",
        "\tthe identification system.\n",
        "\t---------------------------------------------------------------------\n",
        "\tAttributes\n",
        "\t\t1. fileData_str - indicates where the data is stored. The file is\n",
        "\t\t   generated using the using the Matlab code from Linghui\n",
        "\t\t2. fileData - matrix with all the features of all the users in the DB\n",
        "\t\t3. num_users - number of users in the DB\n",
        "\t\t4. num_features - number of features of each user\n",
        "\n",
        "\tFunctions\n",
        "\t\t1. read_fileData_fromFile - creates a list of with all the user's data.\n",
        "\t\t   Each element in the list is a feature vector of one user\n",
        "\t\t2. convert_str2int_matrix - converts the list of feature vectors to a\n",
        "\t\t   matrix that can be used as a mathematical object\n",
        "\t---------------------------------------------------------------------\n",
        "\t\"\"\"\n",
        "\n",
        "\t# Initialization\n",
        "\tdef __init__(self, file_path):\n",
        "\t\tself.file_path = file_path\n",
        "\t\tself.initialize()\n",
        "\n",
        "\tdef initialize(self):\n",
        "\t\tfileData_str = self.read_data_fromFile()\n",
        "\t\tself.fileData = self.convert_str2int_matrix(fileData_str)\n",
        "\t\tself.num_entries = self.fileData.shape[0]\n",
        "\t\tself.num_features = self.fileData.shape[1]\n",
        "\n",
        "\t# Functions\n",
        "\tdef read_data_fromFile(self):\n",
        "\t\tfileID = open(self.file_path,'r')\n",
        "\t\tfileData = []\n",
        "\n",
        "\t\tfor line in fileID.readlines():\n",
        "\t\t\tline = line.strip()\n",
        "\t\t\tfileData.append(line.split(','))\n",
        "\n",
        "\t\tfileID.close()\n",
        "\t\treturn(fileData)\n",
        "\n",
        "\n",
        "\tdef convert_str2int_matrix(self, fileData_str):\n",
        "\t\tfileData_int = []\n",
        "\n",
        "\t\tfor vec in fileData_str:\n",
        "\t\t\tfileData_int.append(list(map(int,vec)))\n",
        "\n",
        "\t\treturn(np.array(fileData_int))\n",
        "\n",
        "\n",
        "\tdef get_info(self):\n",
        "\t\tprint(\"The data is stored in %s\" % self.file_path)\n",
        "\t\tprint(\"There is a total of %d entries\" % self.num_entries)\n",
        "\t\tprint(\"Each entry has %d features\" % self.num_features)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UY3fOE2WRrH-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### evaluate_performance.py"
      ]
    },
    {
      "metadata": {
        "id": "Y7guAkUPRrXV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def findIndex_obs(cluster_ind_train, cluster_ind_test, testData):\n",
        "\tdecoded_db_path = DEFAULT_MAIN_FOLDER + 'decompressed_db_500.txt'\n",
        "\tdecoded_db = dataReader(decoded_db_path)\n",
        "\tdecoded_db_data = decoded_db.fileData\n",
        "\n",
        "\testimated_ind = []\n",
        "\n",
        "\tfor ind_test, obs_test in enumerate(cluster_ind_test):\n",
        "\n",
        "\t\tisEstimated = False\n",
        "\t\tbest_ind = 0\n",
        "\t\tbest_mutual_info = 0\n",
        "\t\tobs_vec = testData[ind_test, :]\n",
        "\n",
        "\t\tfor ind_train, obs_train in enumerate(cluster_ind_train):\n",
        "\t\t\tif obs_test[0] == obs_train[0] and obs_test[1] == obs_train[1]:\n",
        "\t\t\t\tentropy_obs = find_entropy(obs_vec)\n",
        "\t\t\t\tentropy_usr = find_entropy(decoded_db_data[ind_train, :])\n",
        "\t\t\t\tcross_entropy = find_cross_entropy(obs_vec, decoded_db_data[ind_train, :])\n",
        "\t\t\t\tmutual_info = entropy_obs + entropy_usr - cross_entropy\n",
        "\n",
        "\t\t\t\tif isEstimated is False:\n",
        "\t\t\t\t\tbest_ind = ind_train\n",
        "\t\t\t\t\tbest_mutual_info = mutual_info\n",
        "\t\t\t\t\tisEstimated = True\n",
        "\n",
        "\t\t\t\tif (isEstimated is True) and (mutual_info > best_mutual_info):\n",
        "\t\t\t\t\tbest_ind = ind_train\n",
        "\t\t\t\t\tbest_mutual_info = mutual_info\n",
        "\n",
        "\t\testimated_ind.append(best_ind)\n",
        "\n",
        "\treturn estimated_ind\n",
        "\n",
        "\n",
        "def missclassification_error(original_ind, estimated_ind):\n",
        "\terror_rate = 0\n",
        "\n",
        "\tfor k, est_ind in enumerate(estimated_ind):\n",
        "\t\tif original_ind[k] != est_ind:\n",
        "\t\t\terror_rate += 1\n",
        "\n",
        "\treturn float(error_rate / len(estimated_ind))\n",
        "\n",
        "\n",
        "def find_entropy(sequence):\n",
        "\tprob = np.zeros(2)\n",
        "\tprob[1] = float(len(np.nonzero(sequence)) / len(sequence))\n",
        "\tprob[0] = 1. - prob[1]\n",
        "\treturn -1 * np.matmul(prob, np.log2(prob))\n",
        "\n",
        "\n",
        "def find_cross_entropy(seq_p, seq_g):\n",
        "\tprob = np.zeros(4)\n",
        "\tlength_seq_p = len(seq_p)\n",
        "\tfor l in range(length_seq_p):\n",
        "\t\tif seq_p[l] == 1 and seq_g[l] == 1:\n",
        "\t\t\tprob[0] += 1\n",
        "\t\telif seq_p[l] == 0 and seq_g[l] == 1:\n",
        "\t\t\tprob[1] += 1\n",
        "\t\telif seq_p[l] == 1 and seq_g[l] == 0:\n",
        "\t\t\tprob[2] += 1\n",
        "\t\telse:\n",
        "\t\t\tprob[3] += 1\n",
        "\n",
        "\tfor k in range(len(prob)):\n",
        "\t\tif prob[k] != 0:\n",
        "\t\t\tprob[k] = prob[k] / length_seq_p\n",
        "\t\telse:\n",
        "\t\t\tprob[k] = 1\n",
        "\n",
        "\treturn -1 * np.matmul(prob, np.log2(prob))\n",
        "\n",
        "\n",
        "def save_errorRate(error_rate, net_dimension_x, net_dimension_y, num_users, num_features, num_epochs,\n",
        "\t\t\t\t   learn_rate, fileName=MAIN_PATH+'errorRate_SOM.txt', print_config=False):\n",
        "\t\n",
        "    msg = \"error={0} [dim_x={1},dim_y={2},num_users={3},num_features={4},epochs={5},learn_rate={6}]\\n\".format(\n",
        "\t\t\terror_rate,\n",
        "\t\t\tnet_dimension_x,\n",
        "\t\t\tnet_dimension_y,\n",
        "\t\t\tnum_users,\n",
        "\t\t\tnum_features,\n",
        "\t\t\tnum_epochs,\n",
        "\t\t\tlearn_rate)\n",
        "    \n",
        "    with open(fileName, 'a') as errorFile:\n",
        "        errorFile.write(msg)\n",
        "    \n",
        "    if print_config is True:\n",
        "        print(msg)\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4P5csr9nSEyy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Test classes"
      ]
    },
    {
      "metadata": {
        "id": "CKG20jKtSMK-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### test_read_Data.py"
      ]
    },
    {
      "metadata": {
        "id": "QcTsSgIjH17M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5707a504-3f31-4124-efca-59f34413f481"
      },
      "cell_type": "code",
      "source": [
        "# Read user data\n",
        "fileData_path = DEFAULT_MAIN_FOLDER + '500 users/data_len_64/users_data_500.txt'\n",
        "print(fileData_path)\n",
        "fileData_test = dataReader(fileData_path)\n",
        "\n",
        "# Print different attributes\n",
        "fileData_test.get_info()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Project - DL identification systems/Training data/500 users/data_len_64/users_data_500.txt\n",
            "The data is stored in /content/drive/My Drive/Project - DL identification systems/Training data/500 users/data_len_64/users_data_500.txt\n",
            "There is a total of 500 entries\n",
            "Each entry has 64 features\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MP_8deceWPeZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### test_evaluate_performance.py"
      ]
    },
    {
      "metadata": {
        "id": "I1gb7tNpWbOM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "0e6997fe-9e67-4111-8bdb-5a48230edbfa"
      },
      "cell_type": "code",
      "source": [
        "p = np.random.randint(2, size=10)\n",
        "g = np.random.randint(2, size=10)\n",
        "print('Entropy of p = %.3f' % find_entropy(p))\n",
        "print('Entropy of g = %.3f' % find_entropy(g))\n",
        "print('Cross entropy p and g = %.3f' % find_cross_entropy(p,g))\n",
        "\n",
        "a = np.arange(10)\n",
        "b = np.arange(10)\n",
        "c = np.arange(10) + np.ones(10)\n",
        "d = np.arange(10)\n",
        "d[0] = 2\n",
        "print('Error rate a and b = %.3f' % missclassification_error(a,b))\n",
        "print('Error rate a and c = %.3f' % missclassification_error(a,c))\n",
        "print('Error rate a and d = %.3f' % missclassification_error(a,d))\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Entropy of p = 0.469\n",
            "Entropy of g = 0.469\n",
            "Cross entropy p and g = 1.685\n",
            "Error rate a and b = 0.000\n",
            "Error rate a and c = 1.000\n",
            "Error rate a and d = 0.100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cFKetgtvW4iM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Main"
      ]
    },
    {
      "metadata": {
        "id": "XPeHRs2SXDK2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Generate train and test data\n",
        "trainData_path = DEFAULT_MAIN_FOLDER + 'users_data_500.txt'\n",
        "testData_path = DEFAULT_MAIN_FOLDER + 'observations_500.txt'\n",
        "usersData_train = dataReader(trainData_path)\n",
        "usersData_test = dataReader(testData_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x7hPBP3IXZik",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Parameters unsupervised algorithm\n",
        "net_dimension_x = 14\n",
        "net_dimension_y = 14\n",
        "num_nodes = net_dimension_x * net_dimension_y\n",
        "num_features = usersData_train.num_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HtvJKM7NXdt7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Train cluster algorithm\n",
        "trainData = usersData_train.fileData\n",
        "unspv_alg = SOM(net_dimension_x, net_dimension_y, num_features)\n",
        "unspv_alg.train(usersData_train.fileData)\n",
        "cluster_ind_train = unspv_alg.predict_cluster(trainData)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "apxnyI2GXfck",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Test cluster algorithm\n",
        "testData = usersData_test.fileData\n",
        "cluster_ind_test = unspv_alg.predict_cluster(testData)\n",
        "estimated_ind = findIndex_obs(cluster_ind_train,cluster_ind_test,testData)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yqpg_xA-W8ag",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "eb5e8be1-70b4-4a19-96a7-308bb61a43b3"
      },
      "cell_type": "code",
      "source": [
        "# Evaluate performance\n",
        "error_rate = missclassification_error(np.arange(usersData_test.num_entries),estimated_ind)\n",
        "save_errorRate(error_rate,unspv_alg.net_dimensions[0],unspv_alg.net_dimensions[1],\n",
        "\t\t\t   usersData_train.num_entries,usersData_train.num_features,\n",
        "\t\t\t   unspv_alg.DEFAULT_NUM_EPOCHS,unspv_alg.DEFAULT_INIT_LEARN_RATE,print_config=True)\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "error=0.998 [dim_x=14,dim_y=14,num_users=500,num_features=64,epochs=200,learn_rate=0.4]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VhqcUNgZa_Wp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}