{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NNE_multiple_simulations.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zhengro/DL-Identification/blob/NNE_Sara/NNE_multiple_simulations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "5Pj2AaWxY4En",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "metadata": {
        "id": "4UZA9QlgYyg0",
        "colab_type": "code",
        "outputId": "50dd87a1-2d96-4317-ae55-c1121615d45b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import numpy.matlib\n",
        "import math\n",
        "import random\n",
        "from numpy.random import random as rand\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout\n",
        "from keras import backend as K\n",
        "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import EarlyStopping\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Y4NqB94tY_a1",
        "colab_type": "code",
        "outputId": "2130d9c8-1cba-4beb-e3e5-fae6a75ea6b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "cell_type": "code",
      "source": [
        "# connect to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8v_KkcSCZEWB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Parameters"
      ]
    },
    {
      "metadata": {
        "id": "nO-dNiW5ZHot",
        "colab_type": "code",
        "outputId": "666dd96b-bbfa-4a87-eb10-e673994e065d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "# data sequence length(s)\n",
        "N = np.array([64])\n",
        "\n",
        "# num of users\n",
        "M = 1000     \n",
        "\n",
        "# compression rate\n",
        "R = 0.5 \n",
        "\n",
        "# crossover prob.\n",
        "bsc_err = 0.1      \n",
        "\n",
        "# number of learning epochs\n",
        "epochs = np.arange(3000, 4000, 1000)\n",
        "\n",
        "# each list entry defines the number of nodes in a hidden layer\n",
        "design = np.array([[1024, 1024], [512, 512]])    \n",
        "\n",
        "# size of batches for calculation the gradient\n",
        "batch_size = 100       \n",
        "\n",
        "#optimizer \n",
        "optimizer = 'nadam'     \n",
        "\n",
        "# loss function \n",
        "loss ='binary_crossentropy'\n",
        "\n",
        "# test on how many observations per user\n",
        "obs_per_user = 100\n",
        "\n",
        "# droptout value(s)\n",
        "dropout = np.array([0])\n",
        "\n",
        "# name of the file to save the results to (optional)\n",
        "filename = ''\n",
        "\n",
        "print('epochs:', epochs)\n",
        "print('dropouts:', dropout)\n",
        "print ('N:', N)\n",
        "print('hidden layers:', design)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epochs: [3000]\n",
            "dropouts: [0]\n",
            "N: [64]\n",
            "hidden layers: [[1024 1024]\n",
            " [ 512  512]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K_yd2T22DvEu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ]
    },
    {
      "metadata": {
        "id": "oPhus7E1DvUL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# necessary functions \n",
        "\n",
        "# Input to this function can be an array of probabilities\n",
        "def get_entropy(x):\n",
        "    # Calculates entropy for a single value\n",
        "    def get_single_entropy(p):\n",
        "        if p == 1 or p == 0:\n",
        "            return 0\n",
        "        h = -p * np.log2(p) - (1-p) * np.log2(1-p)\n",
        "        return h\n",
        "    dummy_func = np.vectorize(get_single_entropy)\n",
        "    return dummy_func(x)\n",
        "  \n",
        "  \n",
        "def reverse_entropy(y):\n",
        "    xu = 0.5\n",
        "    xd = 0\n",
        "    tst = y\n",
        "    val = get_entropy(tst)\n",
        "\n",
        "    while abs(val - y) > 0.0000001:\n",
        "        if val > y:\n",
        "            xx = xd\n",
        "            xu = tst\n",
        "        else:\n",
        "            xx = xu\n",
        "            xd = tst\n",
        "        tst = tst + (y - val) * (xx - tst) / (get_entropy(xx) - val + np.finfo(float).eps)\n",
        "        val = get_entropy(tst)\n",
        "    if tst < 0.5:\n",
        "        x = tst\n",
        "    else:\n",
        "        x = 1 - tst\n",
        "\n",
        "    return x\n",
        "\n",
        "  \n",
        "def get_bec_bhattacharyya(N, e):\n",
        "    N = int(N)\n",
        "    # e is erasure rate\n",
        "    # N is number of channels (length of the sequence)\n",
        "\n",
        "    n = math.log2(N)\n",
        "    Z = np.zeros(N)\n",
        "    Z[0] = e\n",
        "    temp1 = np.zeros(N)\n",
        "    for i in range(int(n)):\n",
        "        for ii in range(int(2 ** i)):\n",
        "            temp1[2 * ii] = 2 * Z[ii] - Z[ii] * Z[ii]\n",
        "            temp1[2 * ii + 1] = Z[ii] * Z[ii]\n",
        "        Z = temp1.copy()\n",
        "\n",
        "    return Z  \n",
        "  \n",
        "  \n",
        "# compression rate must be 0 < R < 1\n",
        "def get_free_indexes(N, compresseion_rate):\n",
        "    N = int(N)\n",
        "    distortion = reverse_entropy(1 - compresseion_rate)\n",
        "    # print('this is the distortion:', distortion)\n",
        "    bits_per_user = round(N * compresseion_rate)\n",
        "    # print('bits per user:', bits_per_user)\n",
        "    bhat_param = get_bec_bhattacharyya(N, get_entropy(distortion))\n",
        "    # print('this is the bhat param:', bhat_param)\n",
        "\n",
        "    sorted_bhat_param_index = bhat_param.argsort()\n",
        "    # frozen_index = sorted_bhat_param_index[N - bits_per_user:]\n",
        "    free_index = sorted_bhat_param_index[:N - bits_per_user]\n",
        "    # print(frozen_index)\n",
        "    # print(free_index)\n",
        "    return free_index\n",
        "\n",
        "\n",
        "def compose_model(layers):\n",
        "    model = Sequential()\n",
        "    for layer in layers:\n",
        "        model.add(layer)\n",
        "    return model\n",
        "  \n",
        "  \n",
        "def errors(y_true, y_pred):\n",
        "    return K.sum(K.cast(K.not_equal(y_true, K.round(y_pred)), 'uint16'))\n",
        "  \n",
        "\n",
        "# input bits is a M*N array\n",
        "def bsc(input_bits, p_t):\n",
        "  M = input_bits.shape[0]\n",
        "  N = input_bits.shape[1]\n",
        "  output_bits = input_bits.copy()\n",
        "  flip_locs = (rand((M, N)) <= p_t)\n",
        "  output_bits[flip_locs] = 1 ^ output_bits[flip_locs]\n",
        "  return output_bits\n",
        "\n",
        "\n",
        "def get_index_hamming(y, x):\n",
        "    # y is the db (array of vectors)\n",
        "    # x is the observation (one vector)  \n",
        "    x_large = numpy.matlib.repmat(x, y.shape[0], 1)\n",
        "    hamming_distance = np.sum(np.logical_xor(x_large, y), axis=1)\n",
        "    index_of_min = np.argmin(hamming_distance)\n",
        "    return index_of_min\n",
        "  \n",
        "  \n",
        "def get_err_hamming(predictions, db_free, true_labels):\n",
        "  length = predictions.shape[0]\n",
        "  pred_index = np.zeros(length)\n",
        "  \n",
        "  for i in range(length):\n",
        "    pred_index [i] = get_index_hamming(db_free, predictions[i, :])\n",
        "    \n",
        "  num_err = np.count_nonzero( pred_index - true_labels )\n",
        "  err = num_err/length\n",
        "  return num_err, err"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ArzwpFz-Dvgd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# For loop"
      ]
    },
    {
      "metadata": {
        "id": "fhBWpb06DvrE",
        "colab_type": "code",
        "outputId": "4cefe6a3-f476-4dd0-daca-19cb5fa7ca27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "cell_type": "code",
      "source": [
        "# run simulations for multiple cases of N, epochs, hidden layers, \n",
        "# and dropout specified in the paramets section\n",
        "\n",
        "len_N = N.shape[0]\n",
        "len_epochs = epochs.shape[0]\n",
        "len_hidden_layers = design.shape [0]\n",
        "len_dropout = dropout.shape[0]\n",
        "\n",
        "\n",
        "\n",
        "for i_N in range(len_N):\n",
        "  print('/////////////////////////////')\n",
        "  # path to files\n",
        "  base_path = '/content/drive/My Drive/Colab Notebooks/{}_data_len_{}/bsc_err_{}'.format(M, N[i_N], bsc_err)\n",
        "  path_to_orig_data = base_path + '/original_data_{}.txt'.format(R)\n",
        "  path_to_db = base_path + \"/db_{}.txt\".format(R)\n",
        "  \n",
        "  # import data\n",
        "  orig_data = np.loadtxt(path_to_orig_data, delimiter=',', dtype=bool)\n",
        "  db = np.loadtxt(path_to_db, delimiter=',', dtype=bool)\n",
        "  \n",
        "  # results file path\n",
        "  results_file_path = base_path + '/' + filename ; \n",
        "  \n",
        "  # calculate free/frozen indexes\n",
        "  free_indexes = get_free_indexes(N[i_N], R)\n",
        "  num_free_indexes = len(free_indexes)\n",
        "  db_free = db[:, free_indexes]\n",
        "  \n",
        "  # generate observations\n",
        "  observations = bsc(np.tile(orig_data, (obs_per_user, 1)), bsc_err)\n",
        "  true_labels = np.tile(np.arange(M), (1, obs_per_user))\n",
        "  \n",
        "  # scale the database\n",
        "  scaled_db_free = np.tile(db_free, (obs_per_user, 1))\n",
        "\n",
        "  for i_hidden_layers in range(len_hidden_layers):\n",
        "    for i_dropout in range (len_dropout):\n",
        "      \n",
        "      decoder_layers = [Dropout(dropout[i_dropout], input_shape=(N[i_N],))]\n",
        "      \n",
        "      decoder_layers.append(Dense(design[i_hidden_layers, 0], activation='relu'))\n",
        "      decoder_layers.append(Dropout(dropout[i_dropout]))\n",
        "      \n",
        "      for idx in range(1, design.shape[1]):\n",
        "        decoder_layers.append(Dense(design[i_hidden_layers, idx], activation='relu'))\n",
        "        decoder_layers.append(Dropout(dropout[i_dropout]))\n",
        "      \n",
        "      decoder_layers.append(Dense(num_free_indexes, activation='sigmoid'))\n",
        "                              \n",
        "         \n",
        "      model = compose_model(decoder_layers)\n",
        "      model.compile(optimizer=optimizer, loss=loss, metrics=[errors])\n",
        "        \n",
        "      for i_epochs in range (len_epochs):\n",
        "        # train\n",
        "        history = model.fit(orig_data, db_free, batch_size=batch_size, \n",
        "                                  epochs=epochs[i_epochs], verbose=0, shuffle=True)\n",
        "        \n",
        "        # predict\n",
        "        predicted_compressed = model.predict(observations, verbose=1, steps=None)\n",
        "        predictions = np.rint(predicted_compressed)\n",
        "        not_same = np.mean(np.sum(abs(predictions - scaled_db_free), axis=1)) / N[i_N]\n",
        "        \n",
        "        # calculate error\n",
        "        num_err_hamming, err_hamming = get_err_hamming(predictions, db_free, true_labels)\n",
        "        \n",
        "        # print results\n",
        "        print('Case: N={}, hidden_layers={}, dropout={}, epochs={}'.format(N[i_N]\n",
        "                             , design[i_hidden_layers,:], dropout[i_dropout], epochs[i_epochs]))\n",
        "        print('error is:', err_hamming)\n",
        "        print('\\n')\n",
        "        \n",
        "        # optionally, save results\n",
        "#         str = 'num_err_hamming= {}, pe_hamming= {}, N= {}, M= {}, bsc_err= {}, R= {},\\\n",
        "#             hidden_layers= {}, cost_func= {}, not_same={}, epochs={}, dropout={} \\n'.format(num_err_hamming, \n",
        "#             err_hamming, N[i_N], M, bsc_err, R, design[i_hidden_layers,:], loss, not_same, epochs[i_epochs], dropout[i_dropout])\n",
        "        \n",
        "#         with open(results_file_path, \"a\") as myfile:\n",
        "#           myfile.write(str)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/////////////////////////////\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-dc0a3bf879b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         history = model.fit(orig_data, db_free, batch_size=batch_size, \n\u001b[0;32m---> 56\u001b[0;31m                                   epochs=epochs[i_epochs], verbose=0, shuffle=True)\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}