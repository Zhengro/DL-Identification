from __future__ import print_function
import numpy as np
import matlab.engine
import commpy.channels
import functions as func
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from keras.layers import Input, Dense, Dropout
from keras.models import Model
import keras

eng = matlab.engine.start_matlab()

b = 0  # value of frozen bits
R = 0.5  # compression rate
D = eng.hbinv(1-R)  # corresponding distortion

MM = np.array([[500]])  # number of users
# MM = np.linspace(500, 2000, num=4)

bsc_e = np.array([[0.3]])  # crossover probability of BSC
# bsc_e = np.linspace(0.1, 0.4, num=7)

N = np.array([[256]])  # data length
# N = np.array([[64, 128, 256]])

# find error rate
for im in range(MM.size):
    M = MM[0, im]
    err_mtx = np.zeros((bsc_e.size, N.size))
    for ie in range(bsc_e.size):
        e = bsc_e[0, ie]
        err = np.zeros((1, N.size))
        for i in range(N.size):
            L = N[0, i]
            K = int(np.round(L*R))

            Z = eng.get_bec_bhattacharyya(float(L), eng.h_2(D))
            index = np.argsort(Z).ravel()
            index = index[::-1]  # sort Z descend using 2 steps
            free_index = index[L-K:L]+1  # messages indices ‘+1’ is to be adapted for MATLAB
            frozen_index = index[0:L-K]+1  # frozen indices
            free_index = matlab.int64(free_index.tolist())  # preparation
            frozen_index = matlab.int64(frozen_index.tolist())

            data_u = np.zeros((M, L))  # original data
            data_y = np.zeros((M, L))  # compressed data
            data_y2 = np.zeros((M, L))  # reconstruction of compressed data
            data_z = np.zeros((M, L))  # observation
            h_y = np.zeros((1, M))
            h_z = np.zeros((1, M))
            I_xz = np.zeros((1, M))
            w_hat = np.zeros((1, M))

            for j in range(M):
                data_u[j, :] = np.random.rand(1, L) <= 0.5
                temp = eng.sc_SC(matlab.double(data_u[j, :].tolist()), free_index, frozen_index, D)
                data_y[j, :] = np.asarray(temp).ravel()

                temp = eng.channel_encoder(matlab.double(data_y[j, :].tolist()))
                data_y2[j, :] = np.asarray(temp).ravel()

                h_y[0, j] = eng.h_2(float(np.mean(data_y2[j, :])))
            for j in range(M):
                data_z[j, :] = commpy.channels.bsc(np.int_(data_u[j, :]), e)
        # MMI method
        #         h_z[0, j] = func.h_2(np.array([[np.mean(data_z[j, :])]]))
        #         for k in range(M):
        #             h_xz = func.get_type_entropy(data_z[j, :], data_y2[k, :])
        #             I_xz[0, k] = h_y[0, k] + h_z[0, j] - h_xz
        #         w_hat[0, j] = np.argmax(I_xz)
        #         if w_hat[0, j] != j:
        #             err[0, i] = err[0, i] + 1
        # err = err / M
        # err_mtx[ie, :] = err

eng.quit()

# clustering
num_clusters = 5
kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(data_u)
cluster_label = kmeans.labels_.tolist()
centroids = kmeans.cluster_centers_
plt.hist(cluster_label, bins=num_clusters)
plt.title('Histogram with cluster_label')
plt.show()

# construct train set and test set
ns = 1000  # ns is the number of noisy samples for each user
X = np.zeros((M, ns, L))
Y = np.zeros((M, ns, 1))
for j in range(M):
    for i in range(ns):
        X[j, i, :] = commpy.channels.bsc(np.int_(data_u[j, :]), e)
        Y[j, i, :] = cluster_label[j]
x = X.reshape((M*ns, L))
y = Y.reshape((M*ns, 1))
train, test, train_target, test_target = train_test_split(x, y, test_size=0.1, shuffle=True)
# Convert a class vector (integers) to binary class matrix
bin_train_target = keras.utils.to_categorical(train_target, num_classes=num_clusters, dtype='int32')

# parameters
epoch = 100
batch_size = 800
layer1_dim = 120
layer2_dim = 80
layer3_dim = 50
# layer4_dim = 20

input = Input(shape=(L, ))
input_new = Dropout(0.2, input_shape=(L, ))
encoded1 = Dense(layer1_dim, activation='relu')(input)
encoded2 = Dense(layer2_dim, activation='relu')(encoded1)
encoded3 = Dense(layer3_dim, activation='relu')(encoded2)
# encoded4 = Dense(layer4_dim, activation='relu')(encoded3)
output = Dense(num_clusters, activation='sigmoid')(encoded3)

mlp = Model(input, output)
mlp.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])
mlp.fit(train, bin_train_target, epochs=epoch, batch_size=batch_size, shuffle=True)  # validation_split=0.1

preds = mlp.predict(test)
predictions = np.argmax(preds, axis=1)
diff = predictions - test_target.ravel()
errate = np.count_nonzero(diff)/len(predictions)
print("Error rate:%s" % errate)
